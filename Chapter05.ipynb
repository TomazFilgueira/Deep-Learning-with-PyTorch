{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomazFilgueira/Deep-Learning-with-PyTorch/blob/main/Chapter05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAub_L_UUvYz"
      },
      "source": [
        "# Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH2cJ3cDUvY0"
      },
      "source": [
        "# Chapter 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Eo3xmsGLUvY0",
        "outputId": "cda1417d-b62c-4ce3-8f9e-bf1a380ad208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S7IRHAiSUvY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5dfcc36-8eca-42ab-a1f1-44d2338ed51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter5()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter5 import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nNRk3Re6UvY1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms.v2 import Compose, Normalize\n",
        "\n",
        "from data_generation.image_classification import generate_dataset\n",
        "from helpers import index_splitter, make_balanced_sampler\n",
        "from stepbystep.v1 import StepByStep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEnII8j5UvY1"
      },
      "source": [
        "# Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_HwoprRUvY1"
      },
      "source": [
        "## Filter / Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA_5HgC1UvY1"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/conv1.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QRuOWkogUvY1",
        "outputId": "e3a47bc8-96e6-4ad1-8857-d4da1d2e6328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 6, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "single = np.array(\n",
        "    [[[[5, 0, 8, 7, 8, 1],\n",
        "       [1, 9, 5, 0, 7, 7],\n",
        "       [6, 0, 2, 4, 6, 6],\n",
        "       [9, 7, 6, 6, 8, 4],\n",
        "       [8, 3, 8, 5, 1, 3],\n",
        "       [7, 2, 7, 0, 1, 0]]]]\n",
        ")\n",
        "single.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cFw2_TMOUvY2",
        "outputId": "75db7d3a-03dd-41bf-f957-2a099a6450c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "identity = np.array(\n",
        "    [[[[0, 0, 0],\n",
        "       [0, 1, 0],\n",
        "       [0, 0, 0]]]]\n",
        ")\n",
        "identity.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkU6YD0-UvY2"
      },
      "source": [
        "## Convolving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZrbSwRQUvY2"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/conv2.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tNrobXL1UvY2",
        "outputId": "a995c946-d8e5-4863-8aec-3ceacd2a49ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(9)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "region = single[:, :, 0:3, 0:3]\n",
        "filtered_region = region * identity\n",
        "total = filtered_region.sum()\n",
        "total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvUX24TDUvY2"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/conv3.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBNmW3ZsUvY2"
      },
      "source": [
        "## Moving Around (stride flag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NrX-iIkUvY3"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/stride1.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "woX0w1aoUvY3"
      },
      "outputs": [],
      "source": [
        "new_region = single[:, :, 0:3, (0+1):(3+1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoI_C2nlUvY3"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/conv5.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c7wRrFDUUvY3",
        "outputId": "28988731-ffcc-4762-ae5b-a74ddfdc2704",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(5)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "new_filtered_region = new_region * identity\n",
        "new_total = new_filtered_region.sum()\n",
        "new_total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq08h3pwUvY3"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/conv6.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW9FpB-7UvY3"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/conv7.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HeomX6ngUvY3"
      },
      "outputs": [],
      "source": [
        "last_horizontal_region = single[:, :, 0:3, (0+4):(3+4)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9oUw-pUoUvY3",
        "outputId": "82516398-8ea0-4fa8-9dd4-d187910352d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-2080732797.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_horizontal_region\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) "
          ]
        }
      ],
      "source": [
        "last_horizontal_region * identity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3va5WWmUvY3"
      },
      "source": [
        "## Shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuWSrkDCUvY3"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/conv8.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7_jD_gsUvY4"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * (h_f, w_f) = (h_i - (h_f - 1), w_i - (w_f - 1))\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOo1BJdLUvY4"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = (h_i - f + 1, w_i - f + 1)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcV90haUvY4"
      },
      "source": [
        "## Convolving in PyTorch\n",
        "\n",
        "convolutions come in two flavors: `functional` and `module`.\n",
        "\n",
        "There is a fundamental difference between the two, though:\n",
        "\n",
        "* The **functional** convolution takes the kernel / filter as an argument;\n",
        "\n",
        "* the **module** has (learnable) weights to represent the kernel / filter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xeQBlfaUvY4"
      },
      "outputs": [],
      "source": [
        "image = torch.as_tensor(single).float()\n",
        "kernel_identity = torch.as_tensor(identity).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use the functional convolution, `F.conv2d()`, to apply the identity filter to our input image (notice we’re using stride=1 since we moved the region around one pixel at a time):\n",
        "\n"
      ],
      "metadata": {
        "id": "zcKzSWLM2erh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFhKPsObUvY4"
      },
      "outputs": [],
      "source": [
        "convolved = F.conv2d(image, kernel_identity, stride=1)\n",
        "convolved"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we got the same result shown in the previous section.\n"
      ],
      "metadata": {
        "id": "w6o8a7G32os0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s turn our attention to PyTorch’s convolution module, `nn.Conv2d`. It has many arguments; let’s focus on the first four of them:\n",
        "\n",
        "    * in_channels: number of channels of the input image\n",
        "\n",
        "    * out_channels: number of channels produced by the convolution\n",
        "\n",
        "    * kernel_size: size of the (square) convolution filter / kernel\n",
        "\n",
        "    * stride: the size of the movement of the selected region\n",
        "\n",
        "1. **there is no argument** for the kernel / filter itself, there is only a kernel_size argument. The actual filter, that is, the square matrix used to perform element-wise multiplication, is **learned** by the module.\n",
        "\n",
        "2. it is possible to produce multiple channels as output. It simply means the module is going to learn multiple filters. Each filter is going to produce a different result, which is being called a **channel** here.\n",
        "\n",
        "So far, we’ve been using a **single-channel image** as input, and a**pplying one filter** (size three by three) to it, moving **one pixel at a time**, resulting in **one output per channel**."
      ],
      "metadata": {
        "id": "uT-73L5G28XJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TU1uF7sUvY4"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
        "conv(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we set it to 2, it will generate **two (randomly initialized) filters**\n",
        "\n",
        "There are two filters represented by three-by-three matrices of weights\n"
      ],
      "metadata": {
        "id": "bDHjwc-S4tvR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6lb4gBkUvY4"
      },
      "outputs": [],
      "source": [
        "conv_multiple = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1)\n",
        "conv_multiple.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also force a convolutional module to use a particular filter by setting its weights: In this case, **Kernel_identity**\n"
      ],
      "metadata": {
        "id": "GIeiDJvY5Xfe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlvqq-MWUvY4"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    conv.weight[0] = kernel_identity\n",
        "    conv.bias[0] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_WpH01HUvY5"
      },
      "outputs": [],
      "source": [
        "conv(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the weights to get specific filters is at the heart of **transfer learning**. Someone else trained a model, and that model learned lots of useful filters, so we **don’t have to learn them again**. We can set the corresponding weights and go from there.\n"
      ],
      "metadata": {
        "id": "pYJ5X3IL5v2S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dqOhgguUvY5"
      },
      "source": [
        "## Striding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Lrv4-lUvY5"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/strider2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cCZj8pKUvY5"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/strider3.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "notice that using a **larger stride** made the shape of the resulting image even **smaller**.\n",
        "\n",
        "If we are skipping pixels in the input image, there are fewer regions of interest to apply the filter to. We can extend our previous formula to include the stride size (s):\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kWFTtlBd6Vaf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLtVgKGDUvY5"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = \\left(\\frac{h_i - f + 1}{s}, \\frac{w_i - f + 1}{s}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9Yd9jvdUvY5"
      },
      "outputs": [],
      "source": [
        "convolved_stride2 = F.conv2d(image, kernel_identity, stride=2)\n",
        "convolved_stride2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6W7zlfLUvY8"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By adding columns and rows of zeros around it, we expand the input image such that the gray region starts centered in the actual top left corner of the input image. This simple trick can be used to **preserve the original size of the image**.\n"
      ],
      "metadata": {
        "id": "yVWYLOGh79nk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJJqfjnoUvY9"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/padding1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch gives us two options: functional (`F.pad()`) and module `(nn.ConstantPad2d)`. Let’s start with the module version this time:\n"
      ],
      "metadata": {
        "id": "q0-TN60r8H5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OvtNqPjUvY9"
      },
      "outputs": [],
      "source": [
        "constant_padder = nn.ConstantPad2d(padding=1, value=0)\n",
        "constant_padder(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can also do **asymmetric padding** by specifying a tuple in the padding argument representing (left, right, top, bottom). So, if we were to stuff our image on the left and right sides only, the argument would go like this: `(1, 1, 0, 0).`"
      ],
      "metadata": {
        "id": "FgId_JYc8e9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the functional version, one must specify the padding as a tuple. The value argument is straightforward, and there is yet another argument, `mode`, which was set to constant to match the module version above.\n"
      ],
      "metadata": {
        "id": "KvfVVATe9Sy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YuUQQXBUvY9"
      },
      "outputs": [],
      "source": [
        "padded = F.pad(image, pad=(1, 1, 1, 1), mode='constant', value=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Others modes: There are **three other modes**: `replicate`, `reflect`, and `circular`.  "
      ],
      "metadata": {
        "id": "8MumRh-k9WOG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnTIKXKaUvY9"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/paddings.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZGQ_iH4UvY9"
      },
      "outputs": [],
      "source": [
        "replication_padder = nn.ReplicationPad2d(padding=1)\n",
        "replication_padder(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the reflection padder:\n",
        "1. the left and right padded column **reflects the second column** (since the first column is the axis of reflection).\n",
        "\n",
        "1. Similarly, the **top** and **bottom** padded row reflects the **second row**\n",
        "\n",
        "1. The **corners** have the same values as the **intersection** of the reflected rows and columns of the original image."
      ],
      "metadata": {
        "id": "dyPAYNvq97uB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KQ1xm-wUvY9"
      },
      "outputs": [],
      "source": [
        "reflection_padder = nn.ReflectionPad2d(padding=1)\n",
        "reflection_padder(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In circular padding:\n",
        "\n",
        "1. the left-most/(right-most) column gets copied as the right /(left) padded column;\n",
        "\n",
        "1. Similarly, the top-most/(bottom-most) row gets copied as the bottom/(top) padded row.\n",
        "\n",
        "1. The corners receive the values of the diametrically opposed corner:\n"
      ],
      "metadata": {
        "id": "dOCrKneH_D27"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BHYyunbUvY9"
      },
      "outputs": [],
      "source": [
        "F.pad(image, pad=(1, 1, 1, 1), mode='circular')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By padding an image, it is possible to get resulting images with the **same shape** as input images, or **even larger**, should you choose to stuff more and more rows and columns into the input image. Assuming we’re doing symmetrical padding of size `p`, the resulting shape is given by the formula below:\n"
      ],
      "metadata": {
        "id": "ulMUf98U_c2j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ4IjZsqUvY9"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "(h_i, w_i) * f = \\left(\\frac{(h_i + 2p) - f + 1}{s}, \\frac{(w_i + 2p) - f + 1}{s}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZGnFfLiUvY-"
      },
      "source": [
        "## A REAL Filter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example below is an Edge Detector filter from traditional computer vision"
      ],
      "metadata": {
        "id": "VKL6Vwpo_sEl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfbFA0ndUvY-"
      },
      "outputs": [],
      "source": [
        "edge = np.array(\n",
        "    [[[[0, 1, 0],\n",
        "       [1, -4, 1],\n",
        "       [0, 1, 0]]]]\n",
        ")\n",
        "kernel_edge = torch.as_tensor(edge).float()\n",
        "kernel_edge.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGNRC3inUvY-"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/padding2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5xZSq1_UvY-"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/padding3.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeNk2cmfUvY-"
      },
      "outputs": [],
      "source": [
        "padded = F.pad(image, (1, 1, 1, 1), mode='constant', value=0)\n",
        "conv_padded = F.conv2d(padded, kernel_edge, stride=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LMbA7MTUvY-"
      },
      "source": [
        "# Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pooling splits the image into tiny chunks, **performs an operation on each chunk** (that yields a single value), and **puts the chunks together** as the resulting image.\n",
        "\n"
      ],
      "metadata": {
        "id": "zkiqMpONAn-r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcX6_dZMUvY-"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/pooling1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the image above, we’re performing a **max pooling** with a **kernel size of two.**\n",
        "\n"
      ],
      "metadata": {
        "id": "pUMnTvw4BDH7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-8xkfRmUvY-"
      },
      "outputs": [],
      "source": [
        "pooled = F.max_pool2d(conv_padded, kernel_size=2)\n",
        "pooled"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our input image is split into nine chunks, and we perform a simple max operation (just taking the largest value) on each chunk. Then, these values are put together, in order, to produce a **smaller** resulting image.\n"
      ],
      "metadata": {
        "id": "5YCQyB_fBRok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "only **full chunks count**: If we try a kernel of **four-by-four** in our **six-by-six** image, **only one chunk fits**, and the resulting image would have a single pixel.\n",
        "."
      ],
      "metadata": {
        "id": "BMR6u69gCgPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp4DHPAoUvY-"
      },
      "outputs": [],
      "source": [
        "maxpool4 = nn.MaxPool2d(kernel_size=4)\n",
        "pooled4 = maxpool4(conv_padded)\n",
        "pooled4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94N010GkUvY_"
      },
      "outputs": [],
      "source": [
        "F.max_pool2d(conv_padded, kernel_size=3, stride=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MNA4hRaUvY_"
      },
      "source": [
        "# Flattening\n",
        "\n",
        "It simply flattens a tensor, preserving the **first dimension** such that we **keep the number of data points** while collapsing all other dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRu7zh3QUvY_"
      },
      "outputs": [],
      "source": [
        "flattened = nn.Flatten()(pooled)\n",
        "flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQJ9MSKhUvY_"
      },
      "outputs": [],
      "source": [
        "pooled.view(1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvKMEG1pUvY_"
      },
      "source": [
        "# Typical Architecture\n",
        "\n",
        "A typical architecture uses a sequence of **one or more** typical convolutional blocks with each block consisting of three operations:\n",
        "\n",
        "1. Convolution\n",
        "1. Activation function\n",
        "1. Pooling\n",
        "\n",
        "After the sequence of blocks, the image gets **flattened**: Hopefully, at this stage, there is no loss of information occurring by considering each value in the flattened tensor a feature on its own.\n",
        "\n",
        "\n",
        "What those typical convolutional blocks do is akin to **pre-processing** images and **converting them into features**. Let’s call this part of the network a featurizer (the one that generates features).\n",
        "\n",
        "The classification itself is handled by the familiar and well-known hidden and output layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6AqqMvQUvY_"
      },
      "source": [
        "## LeNet5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEb_ThQXUvY_"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/architecture_lenet.png?raw=1)\n",
        "\n",
        "*Source: Generated using Alexander Lenail's [NN-SVG](http://alexlenail.me/NN-SVG/) and adapted by the author. For more details, see LeCun, Y., et al (1998).  [Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). Proceedings of the IEEE,86(11), 2278–2324*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapting LeNet-5 to today’s standards, it could be implemented like this:\n"
      ],
      "metadata": {
        "id": "zfagEqG9GpCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SWAtNScUvZA"
      },
      "outputs": [],
      "source": [
        "lenet = nn.Sequential()\n",
        "\n",
        "# Featurizer\n",
        "# Block 1: 1@28x28 -> 6@28x28 -> 6@14x14\n",
        "lenet.add_module('C1', nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2))\n",
        "lenet.add_module('func1', nn.ReLU())\n",
        "lenet.add_module('S2', nn.MaxPool2d(kernel_size=2))\n",
        "# Block 2: 6@14x14 -> 16@10x10 -> 16@5x5\n",
        "lenet.add_module('C3', nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\n",
        "lenet.add_module('func2', nn.ReLU())\n",
        "lenet.add_module('S4', nn.MaxPool2d(kernel_size=2))\n",
        "# Block 3: 16@5x5 -> 120@1x1\n",
        "lenet.add_module('C5', nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\n",
        "lenet.add_module('func2', nn.ReLU())\n",
        "# Flattening\n",
        "lenet.add_module('flatten', nn.Flatten())\n",
        "\n",
        "# Classification\n",
        "# Hidden Layer\n",
        "lenet.add_module('F6', nn.Linear(in_features=120, out_features=84))\n",
        "lenet.add_module('func3', nn.ReLU())\n",
        "# Output Layer\n",
        "lenet.add_module('OUTPUT', nn.Linear(in_features=84, out_features=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 used three convolutional blocks, although the last one does not have a max pooling, because the convolution already produces a single pixel. Regarding the number of channels, they increase as the image size decreases:\n",
        "\n",
        "* input image: single-channel 28x28 pixels\n",
        "* first block: produces six-channel 14x14 pixels\n",
        "* second block: produces 16-channel 5x5 pixels\n",
        "* third block: produces 120-channel single pixel (1x1)\n",
        "\n",
        "Then, these 120 values (or features) are **flattened** and fed to a typical **hidden layer** with 84 units. The last step is, obviously, the output layer, which produces ten logits to be used for **digit classification** (from 0 to 9, there are ten classes)."
      ],
      "metadata": {
        "id": "Q4hAijSpG21a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQpSsQx4UvZA"
      },
      "source": [
        "# A Multiclass Classification Problem\n",
        "\n",
        "let’s keep it as simple as possible and build a model to classify images into **three classes.**\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA54AAADMCAYAAAAbIXyGAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAADoeSURBVHhe7d0H3I31/8fxj3ZpT2lpLxXZu58QmWWGsktWpEHJCCUlCmUlOw0tkk1oWJXin0KU0Z5USpz/eX/u67odt3vhvkS9no/HedznGq5zrusc55z39f18v1e2WJwBAAAAABCRg4K/AAAAAABEguAJAAAAAIgUwRMAAAAAECmCJwAAAAAgUgRPAAAAAECkCJ4AAAAAgEgRPAEAAAAAkSJ4AgAAAAAiRfAEAAAAAESK4AkAAAAAiNQ+DZ5///23NWvW1IoXL2avvvpqMBcAABxoRox41po3b27ly19n999/XzB39+h3QZ8+faxhwwZWpsy19thjjwVLorVx40br0qWz1a5dy665ppTNmjUrWHLg+vrrr61Hjx6+T6VKlbQ5c+YESwBg/7BPg+fmzZtt6dKltmXLFlu7dk0wFwAAHEhisZi9994CW7bsY/v+++9t69atwZLdo3/366+/xH8TrLWff/7Ztm3bFixJ3+eff26DBw+2mTNnBnN2z6ZNm+yPP/6w1atX+2+T7du3B0sOXBs2bIi/Hst8n3777bf4nFjSAgDYT2RJ8Hz++ectf/58O93uuefuYOkOxx9/vI0cOcp6937Ubr+9RTAXAADsK5n9zk5PtmzZbNCgQVa0aNFgzp458sgjrVu3By1XrlzBnMwZMmSIDR06JJhKnQJYz549rUqVyvHnWcRbVBs2bGhz5861iy++2Lp37xGs+c/QMU/5OgwfPjxYuvvy5du7fw8AUcuS4Fm5cmXr1atXMGXWqFHj+If9Q8HUzi699FIrXbq0HX744cEcAACwr+zOd3ZmHXTQwcG96KmkdPbsWXb66afbNddcE8zd2ZQpU6x+/Xr2yisv2xFHHGGFChWyU045xVto5859K1jrn3XXXXf7sQ/ppHzjxjumAeDfJkuC51FHHWX58xcIpszOPfdcO/TQQ4OpJCpl0dm9atWqWokSxZPPyr3zzjvWqlUr7yPStWtXGzNmtK+js5ONGjW0L774wtcLqR/GzTfX9+XlypWN/5su9ssvvwRLAQBAejLznR1SgGvcuJGPzaC+kHfd1d5++umnYOkOv//+m913X0dvVaxQobwNGvT0TuWrWfnd/cILz3tJbu3adezgg3cNvPrd8OCD3byMt23btvH1X7S+ffvZgAEDfflxxx3nf9OicNq+/Z32v/9d4/ut/qcqYQ1pu/369fXfLUWKFI4H+Uq+P2G58bfffmv33nuP97PUv69Vq6aNGDHClyU69dRT7corrwymzC688EL/u7u/i2bMmBEP2UnHVq27qdHx0u8ubadw4UL+nIcNG+Yl09rfxFZXHTOVT+u5a7pQoYL24YcfBFsCgD23T/t4nn32Of5hpn4V4ReS+nT8+OMPPv+NNyZ5+Yy+FPUB/vHHH1ufPjsGGnjzzckeXvUB2rRpMzv55JNt0qRJ1rv3I8EaAAAgK0yYMME6dbo//h39o5eoHn300T5gjb6nU1JYWrlypWXPnt2+++47DzWTJ0/2ZVn53a0xIl555RUv0a1WrVowd2cvvzzB/vrrL8uRI4fVrVsvmGt20kknWY8ePeP70iiYs6tVq1bFn2NTW7BggdWoUdMuu+wyD50KsqGBAwfGw+AYK1GipD3+eF8777zzfH+S+lWah1b1PW3RoqU/np6LjkFm7c7vIj1uhw73xp/3ynigLxcPiyWCJTt7+OGH7KmnBtr555/v+/f777/7yQEdy9y5r/D9CE8+lC1b1sP5WWed5a/52LFjLU+evL4MAPbGPgue+vDSGTz180x0/fXX20033RRMmb366ms2btxzVq9e0pfFp59+6n9lwIAB/rdDh45Ws2ZNu+225j6tLwgAAJB1FNzy5Mljbdu288BYvnx5n79mza6DAxYuXNhefPElv6kEVmbMmO5/s/K7e+LE131goCpVqvjvitR89NFH/jdXrnPtoIN2/pmjfTjmmGOCqV0pOF999dVWv/7N1rJly+RS2MR9/uijpf5XQfCiiy6y1q1be0hVWNO8Tz75xJerNVHHRWNaVK9ew+dlRmZ/F2n7CpPSrNmt1rVrt3jAv8enE61bt86vJKDfX507d/GW4ipVqvqyBQve878lS5b0fy86IdCzZw8fwOmRR3rbBRcktcQCwN7apy2eGdFgBSeeeKLfP/30nP73zz//9L8are2bb77x+yr7UQmMzirKIYcc4n8BAEDWKFasmJeoqkJJpaJhUNy+fdeRZ9UiJ4cddphddVUev6/v7Kz87lbQGj9+vP9WqFNnRzBLSS2GotbR3aXQ/OST/T1wjx//nPcRFT126Npry/jfSZMmxkNiBXvmmeEeCvW81GqoECePPfaol8uuWLHCKlWq5PN2V3q/i3RJGJX1ilpm0/LBB0llsjou115b2l+D0aNH+bzEEuvrrrvOW2nVcquW1Lvuusv7xgJAVok8eOqD/9VXX0n+oNxTiV8gOgM3aNDg5JtKRAAAwN5J/M5W0FSw6tixg82fP89bGjNDYUlUDpuV391vvz3f+zeWKFHCy0DTcsIJJ/jflSs/Sw6hmaWQXKNGdWvVqqW3NIahOZFCpq43mjt3bi8fnj59mt1yy81ekiwaJEitxGox1vgWCnnaXlZTt6VQcMhTFb4G6lOaePx1C1ufQ7quauijjz4O7gFA1og8eGrkOV3QODzjtqfOOOOM4J467n9j+fPnT76p1AUAAOydxO/sAQP6e7hR/85hw57xPoQZUcvgJ5/8n9/Pmzdvln53jxs3zv/WrVvX/6Yl7Oeolru7777L+2iGJ78VnsNS3NQ899xztn79+qDFc7y3AKaklsDNm3+zESNG2tChw7y1V9v97LNPvV+mBgAqXry4vf76xPjjJ5W+6jHV1zMrKUiGPvzwQ/+b2mOEr4Ge29FHZ9/pNQjLokXluEOGDLabbqprF1xwgfdL1bVSASCrZFnw/L//S/qiEY1ep34HKjMJP7TU90Ej4YUfil9//VVyR/zEM5Lh/fBMns6+6UtAw6FXqHC9z+vTp4/Vq1fX2rVr66U7LVtyTVAAADIrM9/ZYcvltGnT7PHH+8SD1Os+retjPv30U96SFramLV++3B59tLc1a9bM1q5d662OCjCZ+e7W6LZqGRT9Nvjhhx/8fiIN+rNw4UIf+TVxRN7UqH/kJZdc6vcVoDUqbbFiRX00V5WZ9ur1sC9TGXBI+6R9Cff5s88+81bNJ598wqdFz1/9HtX6q/lqGdbzVT/SY4891h9T5a/q49m58wPx4zrTR/sVhXCVISdSa6keN7Ro0SL79ddf/X5mfhfpMXXtTtGIta1bt/JLyIQWLFjoZb4KmGohVsm0TiI0a9bUR6696aY6ya+3BkPSAEQ6GXDHHXd4f08dC10rVdd9zerQDOC/6eCuGqt7L+nDt2PHjsGU+ZeOPux1llEfoupD0KpVa+/rEIZNfRjqy0YfqOHIdjpTqn+jD28NRS76gNVZuMaNm3hfgy1b/vCzdvrQVZBV3wcNAJAjx46zdgAAIHWZ+c5u1+5Ou/jii31eGNC6d+/hgVUhUQGwWLHiviz8Ll+6dGn8O36zl8I+9NDDXmoq6X13n3ZaDr8Ei5bJl19+GX8+a+K/Fyr4dGjgwAE+qI4G8tHzSo+ef8WKFZNbIsPfHQrTRYoU8etnbty4IXmwHnn//SU+6JAGCdLjfPWVAvD38cdrE99Odvvii7X+7/W8fv31F1+uy5ioHPmKK66wbt0etDPPPNPD5erVq/yYvvnmmx4sNUrs/fd38tLjRF26dLFx48YGU2bz5s3zfq/admZ/FylUrlnzuYd1DWyka7SqFVMjDCskH3roId4CXLRoUR9tWOtpsCG9TnoNNWiU+qpq1F49lpYr2I8aNcpfB3nnnbfthBNO9NJiANgb2eIfNDt6zAMAAOxHFGrV11Sj2E6a9MYuLYcAgAMDwRMAAAAAEKn96nIqAAAAAIB/H4InAAAAACBSBE8AAAAAQKQIngAAAACASBE8AQAAAACRIngCAAAAACJF8AQAAAAARIrgCQAAAACIFMETAAAAABApgicAAAAAIFIETwAAAABApAieAAAAAIBIETwBAAAAAJEieAIAAAAAIkXwBAAAAABEiuAJAAAAAIgUwRMAAAAAECmCJwAAAAAgUgRPAAAAAECkCJ4AAAAAgEgRPAEAAAAAkcoWiwvup+mZZ4YF9wAAAAAA/3ZNmjQN7mWNTAXP9evXB/cAAAAAAP92Z555ZnAva2QqeAIAAAAAsKfo4wkAAAAAiBTBEwAAAAAQKYInAAAAACBSBE8AAAAAQKQIngAAAACASBE8AQAAAACRIngCAAAAACJF8AQAAAAARIrgCQAAAACIFMETAAAAABApgicAAAAAIFIETwAAAABApAieAAAAAIBIETz30JYtW4J7wP7hjz/+CO4BAAAA+5d/RfB89NHeVq1aVatRo7pPf//993bPPXdbhQrlrUeP7j4vI6+++qrVrXuTlSpV0r766qtgbupWrVplderUtl9//TWYs2/dcUcbu+GGarZ169Zgzv5td47tv9WIESPs2mtL24oVK4I5ey619/fff/9tNWvWsGefHe7TAAAAwP4kS4Lnxx9/bM2b3+bhr0SJ4lagQH4PGbfe2swWLVoUrBWdUqWusQsvvDC5FfKII46wKlWq2lFHHeU/yDPj0ksvtZIlS9lvv/1msVgsmLurX375xdq1a2tt2txhxx57rPXr18/y58+XfHvmmWeCNc3vJy7r3fuRYMne2bRpk4febdu2BXP2b5k9tvL+++9b+/Z3Wvny11nhwoWscuVK9sgjj3jYCr366ivWtGkTq1SpohUrVtTX0/0HH+xmX3/9dbDWzlq2bJH8Omj9UaNG+Xy9Z/Rvw2Vt2rT2+VlN+67X7c8//wzm7LnU3t+HHHKIde3azYYMGWJTpkzxeQCQnpUrV1qrVi39e/uaa0pZx44dd/qsBQAgS8WDwF6L/3iPLVy4MFaoUMFYnz6PxX744YfYkiVLYvXr14vFg0Fs/fr1wZrRGThwQKxixeuDqSSNGzeKdenSOZjK2HvvvRfLl+/q2IYNG4I5u+ratWusU6dOwVQsFv/hH5s5c6b/u+nTpwdzk8SDYezdd9/xZa+99lowd+9puzrm/4Rx48bGOnToEExlXmaOrbat99ATT/SLLVu2LLZy5WexsWPHxkqVKhm77rpysa+++srXi4f/2IQJE3x7s2fP9vmzZ8/y179atap+fFLSPG1X/+b//u//grk7VK9+Y+zxx/sEU3suvePzxx9/BPeyRmrv72HDhsVKliwRiwfwYA4A7Grt2rX+2frAA51iy5cv98/S8uWvi9WpUzu2devWYC0AALJOlrR4Hn744VagQAHLli2bt8aceOKJdvXVV9tDDz3sLUrx0BGseWCLf1Hb5MlvWIsWLYI5ZgcffLCdcsopfl8tUIkOOuggO/nk1JftDW1Xx/yf8O6779q2bZlrRd4d8TBoffv2tXho89bkyy+/3C644EKrW7euPfXU0/bTTz8ltxirpVkt3HL00Udbjhw57Jpr/metWrW2devW2ZdffunLEumY3XDDjX5/8eKdW+FXr17tr+3111cM5uy59I6P/m9E7eabb7ZjjjnGXnrppWAOAOxq4MAB/tmpSonLLrss/hl6jd17773eCjpnzpxgLQAAsk6kfTzPOussLwH8+eeffXr58uXWtm1bL+spXfp/9vDDD/l8mTZtmjVs2MBLdLV+p073W/HixWz06KSSyKlTp1qDBrdY0aJFvG+b+g3uCZVpVq9+oxUpUthuvPEGf9zMeu211yxPnjx2+umnB3P2zPbt223EiGe9NDl8HsOGDUsum3z66ae8v6rKQz///HNr3LiRH5cPP/zQnnzyCX/+Ojahtm3vSC4VTbwpzElGjzdo0CB/vObNm9s777yTfJxV5jpjxnRfRyWseh4KVrNmzUp+jAULFvjy9F7bzBg3bpzlzJkz/hxvCObsoB9FZcuWs3nz5tnmzZuDubvavHmTnwg4/vjjgzk70/tR20r5o2rmzJmWK1cuu/jii336r7/+sn79+nq5r0p5td+ffvqpL0vrtdE20jo+U6a8afXr17OSJUv4axiKxWI2cuTI5NelVq2avo+i/rt6jcJltWvXsmXLPvZl6TnssMP8WE2c+HqGZc0A/pv0GafPGn1W6KRcqHjxEnbkkUfa4sWLgzkAAGSdSIOnWp4Ubi644AJbv36998u78sor4z/Ep9qdd7a3CRMm2NKlS31d/egvXry4qS+cftSfccYZ1qzZrXbRRRf7D/fOnR+wFi1axoPi9Hi4KeEDCoXBKbMUXvVjvnv3Hr6dMmXK2AMPdLIvvvgiWCN9Cxcu8JbdvdW7d+94MJhoDz7YPR5mX48HmZo2dOiQeAB82pcXLVrMLrzwIg87CuBly5aNh83qduaZZ9p115W3q666yn84hNQa+MQTT9r8+W/b22+/Y1dccYW3OIdBKuPHK+rrfvTRUhs1amT8+Ley4cOftXPOyWXdunXz1+Skk06Kh8le/tjFihWzN96Y7Lf8+fNn+Npmhloh8+XLF0ztqmDBgh6k1DKZ6O+/t/qARS+88EI8lD/pzz2t4Cnly1eI7+dH9uOPPwZzzMN1hQrXB1Nmjz32qK1atdqefXaEH6+jjspud999lwf4tF4bBdq0jk+ePHmtUqVK9vvvv8e3viMM9u37uL3++mvWo0cPf5yzzz7btynq16rwqNd18uQ37dBDD42H3qTXKyMKy+qnpdZfAEhJ33k6uXXeeecFc5LoRLEqeL75JvW+8gAA7I3Igqdagzp37my5c+f2QKny26pVq1nNmjUte/bsHvpk48YN/vecc87xckm56aab7PbbW1iDBg2sUKFCHlxvuSXpvkorNVCNBmlJDA+ZobBVr149DwkqR2zcuIkP0JPZgKQv61NPPS2Y2tVdd7X3lsjEm1oPE3377bfxUPaStW9/lwe1U0891ctJK1eubM8995yHG4UXtaxqAKHevR+NH4+6Xn568skne0C87LLLg60lKVfuOitSpIiXcqr1UkFQZc5q/cvM42l+3rxXe5AfMGCgh7xLLrnEmjRp4pfoWLfuSw8+p512mv9Vma/u66bHyOi1zYieww8//GDHHXdcMGdX4bKUJxtat24df+wqNmbMaHv88cfj75Odj3dK5cqV85LwsGVRQVbv1fLly/u0jpda09u0aeMt2zrmOl4bN270gJvWa6N10zo+KmcrXLiIbz+kYDh+/Hh/XXLnvsJfl6ZNm3mrs+hkQLNmzfz/xQknnGAFChS0DRsydzxPOy2H/9W+AEBK4Yjs+rxOSfP0vQgAQFbL8uCpkkkFLrWAaTRTBRn9+FYfR/Xf0xeayhEnTZro66fWaqmWtkTq69eyZUsPVHPmzLb33nvX5+9Oi6eCqkJGnz59kssg9TwlM33v9LzVXzW9vpUdOnSMh7nxO90UThKpJFUtdwp2iRSq9Ry/+eabYE5S/1G1cmZEYVrlUvPnz7exY8dYjx49PTDJ7jye6Ix3SIFHtN/p2Z3XNjV67tpXjRiclrBcO+xPG3r66UF2//2dPGQdd1zaLZ0hHRe1Qup9JDNmzPAWYrWwy+efr/YgrMu/hO+TcKTbxPdJZl+btKhsVo+jkyAhvUYKslK6dGnvc6qWVY1Su3LlZ5k+niqVEz1HAEjp4IOTvvp///03/5tI3RmOPTbtk4AAAOypLA+eGsDlzTen2KxZs32gArVQir7M7ryznd18c/348snegpRZGuygTp068XBzr5eThiFkd4QX11cQXLx4yU43tYJlRD/iFTrTG2peoUb9CBNvavVKFJ5JTiq73CEMC4n9bXaHAmSXLp2tSZOmHipDUT1eor15bUO5cp1rS5YsSbNfosqcFfTCgJioSpUqHuBUpppRSBaV2y5cuNDXVfDUdGjz5qQfYjNnztrlfaJy46wShsjwfZmS+h5XrHi9PfFEPy+Bzsx+hb777jv/mzKkA4CEVREbN+58XWV9zqiyQ1VGAABktSwPnkceeYSXsaakfpr6sf/SSxO8r6H6a2aWroepfntjxoyNh4tOVrlylWBJ5unfKwSrRW5PnXvuebZixYpgas+odFLCQXlC2m5Ynrm7FGLuu6+j9z1UeaZosBv1Eczqx1OZ6vbtO4fDvXltQxUqVPAWbZUFp6RWWwXE+vXrB3N2puekkRlViqo+qRkNqqPWRK2jfqFr1nzu/TRDYSvmnr5PUjs+qTn77KTXRQMRpUZ9mNV3VCP63nPPvd5nN7NWr17l5c970yIL4N9LXQM0mFvKwfWmT5/mJyv1GQkAQFbL8uCZlkMOOdTPpr7yyiumEfP0w1o/0hctWmSffPKJr6PSQ0nZuqN+c6tWrbS33nrL++ZphFZ56605yQMD6d8qTCSWI2qeBlAI1apVy0e1ffzxPv64H3zwgYcPtahK+PiJ/yaR+i7Omzd3p4F9JGxVTDlfdmwzaZkuA5I/fwEfyl6D2qilUuFjzJgx3q81tH170jZTa+mKxZK2Ge7rwIEDvYxYoVwtaNqm+jxu2rR5Nx5v523Kjue+Y57Kb5cvX+ZBU5fJ0Wu5O69tWsdW/XrV11SD6vTr189H5NVrrtJtDTalfqzVq9cI1rbk/r0Km3psDcyj8Kl97NXr4XTDp05AqN/x4MGDvD+rQlpIfWjVh1Pb0GOr/69C+7BhQ5Nf3/Rem9SOj6Q8vhdddJGX/Pbv/6S/HzXS7ty5c/0ky6ZNm/w9r9b9999/39544w0vt9U+67mEre7aZmrHUycdEsM0AKSkk5Qq+dfntT5vJ02a5F1R9Fmsz1MAALJatvgP9IybZzKgL68HHnggeRRN/fhW62Ria5p+IHfr1tV/YOs6jLVr17HfftvswU+ljvqiGzJksJdtigY4ULmuSlxVuqnRZxVitF6rVq3i4XOkrV+/zsOT+uupf6N2RX0UNWrt8OHPJAdKBY2pU6f5tsaOHet9ELVN9dnTKKXqn/j888/7vwnDhPqUNmrU2O+H9MNf5Y+33dbcGjZs6PM0QMyAAf2T/536W7Zrd6ff1yi6PXv2SC5zVUlo585dfGAHlVDq0iUqG9bIgo0aNYoH26SwcP/993kLXxhoFZL69XvC76tlU8FCy7SvjzzS2y9dkloJ8KhRo70ENaPHGzx4sD377HAPRQo8EydO8gCry9uE/S51jDQarkaEVUmrHk+lxK1bt/HS3vReWw2ck9GxFfU5VSCeNm2qv5d0aZDzzz/fbryxevy477jGpgaJ0nMOaRRZjf4qOmOvUXw1UNX99yeNEJua2bNn+0i13bo9uNO2RcdIYV59iVW2qhLqatWq+eA/6b02ktrx0ejOugxLWFZ788232B133OGvi0YWfvvtt72Pqkp59ZpomQKmwq/m582b118vnVhQy/3tt9/ug0ilfH+rFHzNmjV+6ZZx455Lbu0GgNRo9HFdskyXy9L3tbrKaIC2rOiCAQBASlkSPP9LFFwVFsaOHcdZYexX1ALaokULy537cmvVKmlAJAAAAGB/wGnN3aRLa+jSJa1atUx3oCFgXwuv89m8+e3+FwAAANhf0OK5B1SS+uKLL1ilSpVTHUgJ+CeobE5lw5m5PBAAAACwLxE8AQAAAACRotQWAAAAABApgicAAAAAIFIETwAAAABApAieAAAAAIBIETwBAAAAAJEieAIAAAAAIkXwBAAAAABEiuAJAAAAAIgUwRMAAAAAECmCJwAAAAAgUgRPAAAAAECkCJ4AAAAAgEgRPAEAAAAAkSJ4AgAAAAAiRfAEAAAAAESK4AkAAAAAiBTBEwAAAAAQKYInAAAAACBSBE8AAAAAQKQIngAAAACASGWLxQX307Rt29/BPQAAAADAv93BBx8S3MsamQqePXp0D+4BAAAAAP7tOnV6ILiXNTIVPAEAAAAA2FP08QQAAAAARIrgCQAAAACIFMETAAAAABApgicAAAAAIFIETwAAAABApAieAAAAAIBIETwBAAAAAJEieAIAAAAAIkXwBAAAAABEiuAJAAAAAIgUwRMAAAAAECmCJwAAAAAgUgRPAAAAAECkCJ4AAAAAgEj9I8Hz/ffft2uuKWUvv/xyMOfAsnTpUrvttlutTJlrbdKkScHcrLFp0ya7/voK1qVLZ5/+/vvv7Z577rYKFcpbjx7dfV5WGjCgv9WoUd0qVaoYzNm3li1bZq1atbRy5craK6/s+/fDnrwX33jjDWvQ4Bb73/+usbVr1wZzAQAAAKQlS4Lnxx9/bC1a3G7Vq9/oP+ILFixgJUoUt9q1a1mfPn3sq6++CtZMsmXLFvv99989ZB2ITjvttPi+1bGff/7ZYrHtwdy0ab38+fOleytf/jpfd/v27X5cfvrpJ58+4ogjrEqVqnbUUUfZ33//7fOyUp48eSxXrnPtjz/+CObsW8cff3w8aFf0/d22LeNjmRmbN2+2tm3v8Pdf6dL/8/ejQqLC4sSJrwdrJdmT9+I555xjhQoVOmDfvwCQSCcA33xzcjAFAEA0siR4XnTRRda0aTPbsGFDPCRVsQkTXrb+/fvHw1R5mzFjutWpU9sWL14crG1WtGhRmzt3XjwINAjmHFhy5MhhJUuWDKYy9ttvv/nfESNG2EsvTYgfj5t8Wvd1a9GipR133HE+T39nzpxlffv28+mjjz7aihcv7gEtkcJSlSqVbdWqVcGcPVO8eAm75JJLgqnULVv2sVWuXCmY2nOpbefMM8+MB8/rLVu2bMGcvadj1qpVazv88MPjoTqXTZ06zYYOHWqXXnqpdevWzSZP3vEDa0/ei7lz5/Z/lxnPPTfOOnbsGEwBwP5h27Zt8c/FIXbTTXWsYcMG8c+q54IlAABEI0uCp37gX3311R4esmc/2s466yzLkyevNWrU2F544UU7/fScdt99HXdqVVNL3n/FYYcd5mE1d+4rPAiFIVL3dVMLmm4hrX/wwQcHU6n7/PPPbePGjcFUtJYsed+D7t7Kqu1kxgUXXGDHHnucHXLIIXbCCSfEpy+0Dh062tlnn23z5s0L1koS5Xvx3Xffjf/Ay/qWagDYG/qO0Um6a675n+XMmfM/9Z0MAPhnRN7H85hjjvGyxx9//NHmzp1rK1assObNb/P+kcOHDw/WMtu6dasNGzbMqlWrakWKFPYySbWQJZo/f77Vq1fXl5ctW8aaNm3qfQNV3qsyVJWpjhjxbPI2brzxBt9mWKI6aNAg78/YvHlze+edd7z0smjRIt4Kp5bZROpb2bNnT99+8eLFvJRYJbN74pRTTrGRI0cFU7tS61nXrt28RFT9OdXHs1WrVsHSXY0cOdKPqag1WaW6TZo09mn58MMPfFr7phLeAQMGWCwWC5aaLV68yG69tZmXQ+t1ePHFF4Ilu+rQ4V4bMmSw/fLLL8llwU89NdCXZXS8E6W3nZDKXnv1etifU6lSJa1Tp/vtzz//DJYmefXVV7ykO3y8adOmBUsypjP8Cr6nnHKyT6f1XhQdY7UoFy5cyKpWrWINGzb055TydVm/fp21a9fW3yN6r6hVW77++mtr3LiRB89Zs2Yl7/OCBQt8OQD80266qa7ddtttdsYZZwRzAACIzj4ZXChfvvx20EEH2cqVn/kXXP369T1oKriEHnnkEe9/98QTT9rkyW/aoYceak8//XSw1HwQl/bt7/Qvytmz59jgwUPi4fC7eGisbI8++pi3bPXu3Tu+jYn24IPd7bXXXo+HzJpeSjRoUNJ2FPAuvvhi++ijpTZq1Ehr2bJVPHA8a+eck8tLMMOSWGnWrGk8LP/gLbYKjR999NFeDYZ00kknBfd2deSRR3ofTp1xVt9RBdX0WsmqVq1q99/fye8/9dTT9sYbk5NLc9UK2qZNGz9O06ZNt44d74s//xF+XETBp3Xr1layZCkv8x0xYqRdfXU+X5aaO+9sb7Vq1fYTCHoc3Ro3buLLMjreidLbTmjYsKHeGtyz50P+HpkyZUo8aL4aLDWbOnWqB9vu3Xv4vpUpU8YeeKCTffHFF8EaqVNfTIVxBUQd41tuSSqrTeu9qMdUoO7V6xF76625ds8999qnn66IP9YDftIi0WOPPWbFihX3v2r112BNq1at9Nf74Yd72VVXXRVfXix5n/Pnzx/8SwAAAOC/Y58ET4VIBastW/704KF+hSrPTaRQ2KxZMy85VWlkgQIFvc9oaPny5V7KW6lSJQ8P5513npUoUdJHJS1QoIB9++23NmHCS/FwepddeeWVduqpp1rdunU9mKrvioKF5ufNe7W3yA0YMNAKFizo/RubNGniZcDr1n0ZPJp5X9WGDRt5EDr//PP98TZu3PF8oqDwnC9fPsuZM/2zz3pOJ5yQVK574okn+mBHxx57rE8rZGq/FMpURlWqVCm7/PLLbcmSpD62/fs/aeXLV/DApX+nPpbav7ToOGbPnt1PHGh93XT8M3O8E6W1nURqwW7e/HYfuKdZs1vt3HPP9VbJkEJtvXr17LLLLvP3kYKrWjE1ynBa9P4IW8d1gmH06DF28slJLZ5pvRd1YkKPoZuWKTgmPZdPvX9nIoXLGjVqWOHCRezeezv4PK2n97z2UX+1jXCfMyqhBgAAAP6N9knwVNBLKnE8JZizq9KlS/vopp9++qm3dKl1NLFkUwMYaXr27Nk+rbLUDz74wFswRcFU5aQpB8pRiFG55jfffBPMSaKQF1LQFZV6htQ/9cILL/SWMrXEqsw2tRLS/c1nn31mc+bMSS7t1E0jFirk6TVQkNNItntrd493ZhxxxJHBvSQKiOFrom2q1VujJIf7pfJWSRlgE6kVct68+R6wf/jhew+bGbn44kvsk08+sfXr1/u0HnfdunXx+Rf5dCK1Vof0PlKwTnwfAQAAANhHwVOXW1ELWKFCBYM5u1JfvYoVr7cnnujnLU4pf7wrBGokWV3fUn3t1HcxV65zrHXrNr5cLV+ScvCaMBgoEOwOtQyqb9+YMWNt9erVydvf36lcuHZtjSK8ZKebSm7D/pKZuQRMRrL6eGckHJiqd+9Hd9m3cuXK+bK0qNVRrZF6j+kanBm58cYb/fmrP7H6Dzdq1NBHvS1bNv3HAQAAAJC6yIOnWgnV5y+prPXSYO6uHn20t1WvXt37LKpPnVqqEq1Zs8bee+8979unPqC6BEaPHj29fFPCUWFTDt6iFr6wzDGz9FgaXEZ9/NR3r23bdj4q7f4l6fIjiYMGiVr2Fi5cmGpQVoucguGHH34YzAntvI2UVOKcsnR2T453atvJLJUXq3RYg/XsiSJFili1ajfYQw/19NbM9Lz++uu+f+rfOWnSG96nWJcL2hNJ+5z+8QUAAAD+7bIseKo1TSHol19+9hFh1T9zxowZ8R/sTbwVTiExpPV0SyxdVavU/Plve588tUqp3FYj4SrYaHsrV660v/76ywf4WbJkib399tseRMN+oGoRzZ+/gA0cOMBHqFWpp0YTHTNmjN1+ewtfR8Lgk/jY4bytW5PmhWW4kyZN9JA2evQoLwFW3z2NtKtWvpT/RhS6xo0bl2FJbtjyqP1LGRxFLZLafuIyTWsQnFBYHqxjpec4ZcqbQWtnHQ/Od97ZLn4853lrsy4MrvJbUfjSdSw1oqxG9tVorqNHj/agqpLS1J67HksD9Gg7en10fDN7vBOlth0J9zNlWNbsxEGWatWq5aPaPv54H1u0aJGXWr/wwgv+3khJ75Xfftvsr5VGmJW77rrLn3fr1q2S/01q78XPPvvU31c6tnoMXX5F16FNHNU4DJOJ/y61/dA+L1++zE8G6P2aeD1bAPin6bNS30mqMtJ3EgAAUckW/7G8I93sIQVBBZ1wVFi18ugHtwZkUXlitWrVksOcAoeu6akwKRr4RqPG6ke5LqWhQWvy5tU1QBtZp06dvKVLf3PkON0voyEKE4lPW5e40EBAv/76q5fqKlApJGhAIG2nTJmyvt7gwYPt2WeHe1hQ0J04cZKXcOri2brMh3To0MFHZ1XYVKun1r322mutUKHC8fDc3QfSqVy5ivXt+3jyPqg1rX//AfHQNdr69evn+6P9SknB7o472uw0aJL6tqp8VPQcNLS9RkUVtU4OHTrMunXrmhyU1Oo3deo0H7BGZcczZ870+0WKFI1Pd/H9UsjUqKxhkNSAOCo11euhHxi6vMr06dO8n6xalhVGdRkXbVuhUaW6iXQs27Zt6yO7ajCjmjVr+eVFMjreKaW2HZVPt2nTOjkc6vE12JP2LSyL1TU4x48f7/syduxYPyGg0XvVt7No0WL+moUt36LjqPdKYlAMXxMFX21boxS3aXOHB/CU70Wd1NDlanT8E0uJdWw1KNV3333n71UdP5XkDho02K644or4/tRI7hdap06deNC92x9Hl4XRY+j6tioNL168uK8DAP8kfafpczbxpKZGINdAcQAAZLUsCZ5RUwtS/fr1vIWtffv2wdyk1iaFjj//3OLB7582cOBAe/758R4Mw76OOLAoOOq91rVrV7+wekgBVNcNrVPnJg/dAAAAADJvnwwutLd0Nlblo2rBUjnrl19+6eWluqajLhNSr179YM1/jkLwrFkz/ZIwhM4Dl1pS1ZKpUWz1nlOrsfqV6pqlosv5AAAAANg9B0SLp8ydO9f7I65Z87mXi+pSG7rmZf36N3u/vX/a1KlTvQS0b99+O12qBQcW/XcYNWqUX0Lnq6++8nk5c+b0633efPPNXiIMAAAAYPccMMETAAAAAHBgOiBKbQEAAAAABy6CJwAAAAAgUgRPAAAAAECkCJ4AAAAAgEgRPAEAAAAAkSJ4AgAAAAAiRfAEAAAAAESK4AkAAAAAiBTBEwAAAAAQKYInAAAAACBSBE8AAAAAQKQIngAAAACASBE8AQAAAACRIngCAAAAACJF8AQAAAAARIrgCQAAAACIFMETAAAAABApgicAAAAAIFIETwAAAABApAieAAAAAIBIZYvFBffTtG3b38E9AAAAAMC/3cEHHxLcyxqZCp49enQP7gEAAAAA/u06dXoguJc1MhU8AQAAAADYU/TxBAAAAABEiuAJAAAAAIgUwRMAAAAAECmCJwAAAAAgUgRPAAAAAECkCJ4AAAAAgEgRPAEAAAAAkSJ4AgAAAAAiRfAEAAAAAESK4AkAAAAAiBTBEwAAAAAQKYInAAAAACBSBE8AAAAAQKQIngAAAACASP0jwfP999+3a64pZS+//HIw58CydOlSu+22W61MmWtt0qRJwdyssWnTJrv++grWpUtnn/7+++/tnnvutgoVyluPHt19XlYaMKC/1ahR3SpVqhjM2beWLVtmrVq1tHLlytorr+yf74eVK1farbc2s6JFi1i1alVt7ty5wRIAAAAAmZElwfPjjz+2Fi1ut+rVb/RAWbBgAStRorjVrl3L+vTpY1999VWwZpItW7bY77//7iHrQHTaaafF962O/fzzzxaLbQ/mpk3r5c+fL91b+fLX+brbt2/34/LTTz/59BFHHGFVqlS1o446yv7++2+fl5Xy5MljuXKda3/88UcwZ986/vjj40G7ou/vtm0ZH8vM+O2336xQoYLJx1ahesWKT4Klu0fHpXXrVvHQWdReeOFF/7tx44ZgKQAcuD744ANr3ry5FS9ezK67rpw9+mhv++uvv4KlAABkrSwJnhdddJE1bdrMNmzYEA9JVWzChJetf//+8TBV3mbMmG516tS2xYsXB2ub/3ifO3eeNWjQIJhzYMmRI4eVLFkymMqYgpCMGDHCXnppQvx43OTTuq9bixYt7bjjjvN5+jtz5izr27efTx999NHxHwXFPaAlUnCvUqWyrVq1KpizZ4oXL2GXXHJJMJW6Zcs+tsqVKwVTey617Zx55pnx4Hm9ZcuWLZiz97Jnz25vvTXXrr76at+3iRMnxf9eGizdPZMmTfTn1qBBQ3+u99xzr79+aoV+4YUXgrUA4MCyYMECu/325v59PHz4s1azZk17/vnnbcyYMcEaAABkrSwJnocffrj/yNcP9OzZj7azzjrL8uTJa40aNfZWotNPz2n33ddxp1Y1teT9Vxx22GEeVnPnvsJy5cqVHCJ1X7dzzjnHbyGtf/DBBwdTqfv8889t48aNwVS0lix534Pu3sqq7WSG3l+HHXa4HXLIIXsVaj/55BM744wzd9nG22+/bbFYLJgCgAPLiy++YAULFrRbbrkl+eRxzpw5bdGihcEaAABkrcj7eB5zzDHWtu0d9uOPP3rfuBUrVljz5rd5/8jhw4cHa5lt3brVhg0b5n3oihQp7GW6aiFLNH/+fKtXr64vL1u2TPyLsqn3DVR5r8pQVaY6YsSzydu48cYbfJthieqgQYO8P6NKi9555x1r0OAW77enVji1zCZS38qePXv69lWGpFJilczuiVNOOcVGjhwVTO1KZ5y7du1mmzdv9pY09fFs1apVsHRXI0eO9GMqak1WOWmTJo19Wj788AOf1r6phHfAgAE7haTFixd5n0WVQ+t10A+QtHTocK8NGTLYfvnll+TS1aeeGujLMjreidLbTkgl2L16PezPqVSpktap0/32559/BkuTvPrqK17SHT7etGnTgiV7Jq1j9c0331jDhg1s4sSJvk7ic9bj6nmpLE3zChcu5M8dAA4U5ctXiH+Plg2mkuikp76zAQCIRPxHdpaJ/wCPxcNdMLXDX3/9FStQIH+sf/8nY7/++mts3ry5sZIlS8SGDh0arBGLde/ePValSuXY2rVrY/GQGosHzFg87AVLY7E1a9bEChYsEIsHgdgff/wRW716daxq1SqxJ598IrZw4UJf5+GHH47FQ0Fs6dKlsXhwiI0dO9b/jR5XNP+++zrG4qEldtttt8YWLFgQ++STT2ItW7aMxcNlLB78fD2Jh6nYnXe2i/3000+xVatWxYoVKxp75plngqWxWDwox/Lluzr2+uuvBXMyT/utf5uStrl48eLYLbfcHIuH82BuksaNG8W6dOns9/WcZs2a5dvQPnz99dexeKDzZRs2bIjFA2Vs+vTpsU2bNsXmzJkTi4ej2GuvJT3P9957z1+n0aNH+79bt25drEOHDrHSpf/ny1PScezXr18sHu59fd10/CWj450ove2I3h/xsBl7+umn/DnGQ6rv3/jx44M1YrEpU6bEKla8PrZ8+XJ/Hw0cOMAfT++Z1Oh1bdDglmBqV+kdK71n9RzbtWsbiwfT5Of8888/+189X72Ouv/dd98FWwSAA5O+5/SZO3Xq1GAOAABZa5+ManvooYf64DhbtvzpZ1PVr1DluYnU6tesWTMvOT3hhBOsQIGC3mc0FA8bXu5YqVIlL6M877zzrESJkj5CboECBezbb7+1CRNesvbt77Irr7zSTj31VKtbt65VrlzZnnvuOW+d0/y8ea/2FrkBAwZ6mZH6ADZp0sTLgNet+zJ4NPO+qg0bNvKy2PPPP98fL+pBZVQWmi9fPsuZ84xgTur0nE44Ialc98QTT/TBjo499lifHjlyhO9XmTJlvH9oqVKl7PLLL7clS5L62MZDoZ/prl+/vv879VvU/qVFx1F9Jg866CBfXzcd/8wc70RpbSeRWrCbN7/dChUqFH8v3Grnnnuut5CHhg4dYvXq1bPLLrvM30eNGzexbdu2+SjDeyK9Y6X3rJ7j4YcfkXxfN/XB1V/Rc9D9k08+2acB4ECkiqPu3R+0q666apdWUAAAsso+CZ4Keurbp5LTtJQuXdpHN/30009typQptnLlZzuVbKoPiqZnz57t0ypL1Yh8F198sU8rmMaD9C4D5SjEqCxSpZOJFPJCCrqSWC6p/qkXXnihl1lOnPi6l9mmVkK6v/nss89szpw5yaWhuumSJQp5eg0U5DSS7d7a3eOdGUcccWRwL4kCXfiaaJtr1671UZLD/VIJtKQMsJmV3rECgP8CnSTs3Lmzfffdd9ar1yN71SceAID07JPgqcut6MtNl7hIi/rqVax4vT3xRD/76KOlu/SZUwjUSLK6vqX6/6k/Xq5c51jr1m18uVq+JOXgNUcemRRm1NK2O9QyWLVqFRszZqytXr06efv7O42gW7u2RhFestOtY8f7kvtLZuYSMBnJ6uOdkXBgqt69H91l38qVK+fLMksnN3QSIb1jBQD/djp52L17d3v//SU2cOBT6Z4cBgBgb0UePPUDf9Cgp4Oy1rQvaaGBWqpXr25PPfW0X7JCo+QmWrNmjb333nvxgDrdJk9+0y/H0qNHTy/flHBUWA0Rn0gtfGGZZGbpsTSAj87+PvbYY9a2bTsflXb/knRWWj8cEql0duHChakGZbXsKhh++OGHwZxQ+qOz6gx4ytLZPTneqW0ns1RerHLYd999N5iz5+699x5bv359uscqI9qXrAjwAPBP0Gfxgw8+GP8Mf8+GDBnqI6wDABClLAueak1TCPrll599RFj1z5wxY4Y1bdrEW5YUEkNaT7fE0lX1o5s//23vs/nGG294ua1GwlWw0fZWrlzpF7Z++eWXbcmSJX45CwXRsB+oWkTz5y9gAwcO8BFqVeo5a9YsvybZ7be38HUkDD6Jjx3O27o1aV5YhqtrOCqkjR49ylvJVqz41EfaVStfyn8jCl3jxo3LsCQ3bHnU/qUMjqJAo+0nLtO0+uGEwvJgHSs9xylT3gxa8Op4cL7zznbx4znPW5vffHOyl5RKtWo3xIP7ZB+dVSP7amTh0aNHe/hSKWtqz12PtWnTJt+OXh8d38we70SpbUfC/UwZADV727Ydz6dWrVo+qu3jj/exRYsWeam1rqWp90Zq9G/DEt3wpuOg0HnSSSdleKxE29BrnPKi6toXjdKsY//WW2/5tgHgQPHoo4/6d1zVqtW8ykhdSsKb+vADAJDVssV/9Kff3JUJCoL68a7gI2oN0g9zDQ5Ttmy5eNiplhzmFDh0TU+FSdFgLrrUiEKkLqWhL7y8eXUN0EbWqVMnb+nS3xw5TvfLaIiCX+LT1qVHNBDQr7/+6qW6ClTqk6kBgbSdMmWSBksYPHiwPfvscA9XCroTJ07yEk5dNkOX+ZAOHTpYjRo1PWyq1VPrXnvttVaoUOF4eO7uA+lUrlzF+vZ9PHkfihQpYv37D4iHrtHWr18/3x/tV0oKJ3fc0WanQZPUt1Xlo6LncNttt9mqVUlBSq2TQ4cOs27duiaHK7X6TZ06zQdnUtnxzJkz/X6RIkXj0118vxScdJkTPZ6ef+7cue3eezv466EgpkuGTJ8+zfvJqmVZYVSXcdG2FRpVfppIx7Jt27bx8L3CBzOqWbNW/Jg1zPB4p5TadlQ+3aZNa/v66699HT2+BnvSvilUywUXXGjjx4/3fRk7dqz/WNI1TNUXs2jRYv6ahS3fonCr1/SLL74I5uxM15l95ZVX/X5ax+r000+3Zs2a+nU8Q+H7TF566cV4eH/KTwZosKMHHujsLagAcCDo1auXf46lpM/VUaNG++c5AABZKUuCZ9TUEla/fj1vYWvfvn0wN6nVUqHjzz+3ePD7pw0cONCef368B8OwryMAAAAA/Nftk8GF9pZalVQSqRYzlbN++eWXXuI4bNgwv/RFvXr1gzX/OQrBs2bN9EvCEDoBAAAAYIcDosVT1J9O/RHXrPncy0V1qQ1d87J+/Zu9v+E/berUqV4C2rdvv50u1QIAAAAA/3UHTPAEAAAAAByYDohSWwAAAADAgYvgCQAAAACIFMETAAAAABApgicAAAAAIFIETwAAAABApAieAAAAAIBIETwBAAAAAJEieAIAAAAAIkXwBAAAAABEiuAJAAAAAIgUwRMAAAAAECmCJwAAAAAgUgRPAAAAAECkCJ4AAAAAgEgRPAEAAAAAkSJ4AgAAAAAiRfAEAAAAAESK4AkAAAAAiBTBEwAAAAAQIbP/B7QhFvf4Du/OAAAAAElFTkSuQmCC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7vWjS0HUvZA"
      },
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8muNMH6tUvZA"
      },
      "outputs": [],
      "source": [
        "images, labels = generate_dataset(img_size=10, n_images=1000, binary=False, seed=17) #1000 images, size 10x10 pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ras_uw_CUvZA",
        "outputId": "67019c53-20e8-4877-e0e0-f868f2da953c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x450 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = plot_images(images, labels, n_plot=30) #Format NCHW -n_images, Channels, Height, Widght"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeggwRj4UvZA"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "v-mpeqOoUvZA"
      },
      "outputs": [],
      "source": [
        "class TransformedTensorDataset(Dataset):\n",
        "    def __init__(self, x, y, transform=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.x[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i6QlLlkzUvZB"
      },
      "outputs": [],
      "source": [
        "# Builds tensors from numpy arrays BEFORE split\n",
        "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
        "x_tensor = torch.as_tensor(images / 255).float()\n",
        "y_tensor = torch.as_tensor(labels).long()\n",
        "\n",
        "# Uses index_splitter to generate indices for training and\n",
        "# validation sets\n",
        "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
        "# Uses indices to perform the split\n",
        "x_train_tensor = x_tensor[train_idx]\n",
        "y_train_tensor = y_tensor[train_idx]\n",
        "x_val_tensor = x_tensor[val_idx]\n",
        "y_val_tensor = y_tensor[val_idx]\n",
        "\n",
        "# We're not doing any data augmentation now\n",
        "train_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
        "val_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
        "\n",
        "# Uses custom dataset to apply composed transforms to each set\n",
        "train_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
        "val_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
        "\n",
        "# Builds a weighted random sampler to handle imbalanced classes\n",
        "sampler = make_balanced_sampler(y_train_tensor)\n",
        "\n",
        "# Uses sampler in the training set to get a balanced data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZp9dn2mUvZB"
      },
      "source": [
        "## Loss\n",
        "\n",
        "In multiclass classification the problem is more complex: We need to ask more questions; that is, we need to get log odds ratios for every possible class. In other words, we **need as many logits as there are classes** and sigmoid activation function takes only **one logit**\n",
        "\n",
        "We need another one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP3LfH7SUvZB"
      },
      "source": [
        "### Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIn0tu67UvZB"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "\\begin{array}\n",
        "& z & = \\text{logit}(p) & = \\text{log odds ratio }(p) & = \\text{log}\\left(\\frac{p}{1-p}\\right)\n",
        "\\\\\n",
        "e^z & = e^{\\text{logit}(p)} & = \\text{odds ratio }(p) & = \\left(\\frac{p}{1-p}\\right)\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MIfhBLsUvZB"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{c=0}^{C-1}{e^{z_c}}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LxdpFLtUvZB"
      },
      "source": [
        "In the equation above, `C` stands for the **number of classes** and i corresponds to the index of a particular class. In our example, we have three classes, so **our model needs to output three logits** (z0, z1, z2). Applying softmax to these logits, we would get the following:\n",
        "\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\text{softmax}(z) = \\left[\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IyeCMKKdUvZB"
      },
      "outputs": [],
      "source": [
        "logits = torch.tensor([ 1.3863,  0.0000, -0.6931])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wto9kUuBUvZB",
        "outputId": "5a4c3ff1-8859-4ba8-d280-5d36aa9d18c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.0000, 1.0000, 0.5000])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "odds_ratios = torch.exp(logits) #We exponentiate the logits to get the corresponding odds ratios:\n",
        "odds_ratios"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting tensor is telling us that the `first class` has much better odds than the other two, and the `second one` has better odds than the third. So we take these odds and **add them together**, and then compute each class' contribution to the sum:\n",
        "\n",
        "The probabilities are proportional to the odds ratios. This data point most likely belongs to the `first class `since it has a probability of **72.73%**.\n"
      ],
      "metadata": {
        "id": "YdrpxnRyKD_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XoEYr7XlUvZB",
        "outputId": "2a1b8982-5140-4259-e59a-9cef66dbdfd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7273, 0.1818, 0.0909])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "softmaxed = odds_ratios / odds_ratios.sum()\n",
        "softmaxed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But there is absolutely no need to compute it manually, of course. PyTorch provides the typical implementations: functional (`F.softmax())` and module (`nn.Softmax`):\n"
      ],
      "metadata": {
        "id": "ou4S2TB9mjiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tmRGyuVQUvZC",
        "outputId": "848498f4-a654-460e-be01-93261747b44e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.7273, 0.1818, 0.0909]), tensor([0.7273, 0.1818, 0.0909]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "nn.Softmax(dim=-1)(logits), F.softmax(logits, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`\"Why do I need to take the log of the softmax?\"`\n",
        "\n",
        "The simple and straightforward reason is that the loss function expects `log probabilities` as input.\n",
        "\n",
        "PyTorch provides `F.log_softmax()` and nn.LogSoftmax out of the box.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YHHiIPYLnHsT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TekGLtWIUvZC"
      },
      "source": [
        "### Negative Log Likelihood Loss\n",
        "\n",
        "Since softmax returns **probabilities**, logsoftmax returns **log probabilities**. And that’s the input for computing the negative log-likelihood loss, or `nn.NLLLoss()` for short. **This loss is simply an extension of the binary cross-entropy loss for handling multiple classes.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNnpY9moUvZC"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "\\texttt{BCE}(y)={-\\frac{1}{(N_{\\text{pos}}+N_{\\text{neg}})}\\Bigg[{\\sum_{i=1}^{N_{\\text{pos}}}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_{\\text{neg}}}{\\text{log}(1 - \\text{P}(y_i=1))}}\\Bigg]}\n",
        "\\\\\n",
        "$$\n",
        "\n",
        "See the log probabilities in the summation terms? In our example, there are **three classes**; that is, our labels (y) could be either **zero**, **one**, or **two**. So, the loss function will look like this:\n",
        "\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+N_1+N_2)}\\Bigg[{\\sum_{i=1}^{N_0}{\\text{log}(\\text{P}(y_i=0))} + \\sum_{i=1}^{N_1}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_2}{\\text{log}(\\text{P}(y_i=2))}}\\Bigg]}\n",
        "\\\\\n",
        "\\Large \\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+\\cdots+N_{C-1})}\\sum_{c=0}^{C-1}{\\sum_{i=1}^{N_c}{\\text{log}(\\text{P}(y_i=c))} }}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZj9hw6rUvZC"
      },
      "outputs": [],
      "source": [
        "log_probs = F.log_softmax(logits, dim=-1)\n",
        "log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O47fHvD9UvZC"
      },
      "outputs": [],
      "source": [
        "label = torch.tensor([2])\n",
        "F.nll_loss(log_probs.view(-1, 3), label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUzr1-a1UvZC"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(11)\n",
        "dummy_logits = torch.randn((5, 3))\n",
        "dummy_labels = torch.tensor([0, 0, 1, 2, 1])\n",
        "dummy_log_probs = F.log_softmax(dummy_logits, dim=-1)\n",
        "dummy_log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZkSTv5PUvZC"
      },
      "outputs": [],
      "source": [
        "relevant_log_probs = torch.tensor([-1.5229, -1.7934, -1.0136, -2.0367, -1.9098])\n",
        "-relevant_log_probs.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkWeg0APUvZD"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.NLLLoss()\n",
        "loss_fn(dummy_log_probs, dummy_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgjF_dy9UvZD"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.NLLLoss(weight=torch.tensor([1., 1., 2.]))\n",
        "loss_fn(dummy_log_probs, dummy_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQSKCaGqUvZD"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.NLLLoss(ignore_index=2)\n",
        "loss_fn(dummy_log_probs, dummy_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zXrZPl4UvZD"
      },
      "source": [
        "### Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8_pW8n6UvZD"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(11)\n",
        "dummy_logits = torch.randn((5, 3))\n",
        "dummy_labels = torch.tensor([0, 0, 1, 2, 1])\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn(dummy_logits, dummy_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV0CDYj3UvZD"
      },
      "source": [
        "## Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dXrcHuwUvZD"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "model_cnn1 = nn.Sequential()\n",
        "\n",
        "# Featurizer\n",
        "# Block 1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\n",
        "n_channels = 1\n",
        "model_cnn1.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\n",
        "model_cnn1.add_module('relu1', nn.ReLU())\n",
        "model_cnn1.add_module('maxp1', nn.MaxPool2d(kernel_size=2))\n",
        "# Flattening: n_channels * 4 * 4\n",
        "model_cnn1.add_module('flatten', nn.Flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cR7xwj1dUvZD"
      },
      "outputs": [],
      "source": [
        "# Classification\n",
        "# Hidden Layer\n",
        "model_cnn1.add_module('fc1', nn.Linear(in_features=n_channels*4*4, out_features=10))\n",
        "model_cnn1.add_module('relu2', nn.ReLU())\n",
        "# Output Layer\n",
        "model_cnn1.add_module('fc2', nn.Linear(in_features=10, out_features=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko6qh52bUvZE"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/classification_softmax.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xt1MKYaUvZE"
      },
      "outputs": [],
      "source": [
        "lr = 0.1\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer_cnn1 = optim.SGD(model_cnn1.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJYh0yi8UvZE"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGJtmsodUvZE"
      },
      "outputs": [],
      "source": [
        "sbs_cnn1 = StepByStep(model_cnn1, multi_loss_fn, optimizer_cnn1)\n",
        "sbs_cnn1.set_loaders(train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8AgZOACUvZE"
      },
      "outputs": [],
      "source": [
        "sbs_cnn1.train(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkBvMlamUvZE"
      },
      "outputs": [],
      "source": [
        "fig = sbs_cnn1.plot_losses()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHxlLEtLUvZE"
      },
      "source": [
        "## Visualizing Filters and More!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQthXSuhUvZE"
      },
      "outputs": [],
      "source": [
        "@staticmethod\n",
        "def _visualize_tensors(axs, x, y=None, yhat=None,\n",
        "                       layer_name='', title=None):\n",
        "    # The number of images is the number of subplots in a row\n",
        "    n_images = len(axs)\n",
        "    # Gets max and min values for scaling the grayscale\n",
        "    minv, maxv = np.min(x[:n_images]), np.max(x[:n_images])\n",
        "    # For each image\n",
        "    for j, image in enumerate(x[:n_images]):\n",
        "        ax = axs[j]\n",
        "        # Sets title, labels, and removes ticks\n",
        "        if title is not None:\n",
        "            ax.set_title(f'{title} #{j}', fontsize=12)\n",
        "        shp = np.atleast_2d(image).shape\n",
        "        ax.set_ylabel(\n",
        "            f'{layer_name}\\n{shp[0]}x{shp[1]}',\n",
        "            rotation=0, labelpad=40\n",
        "        )\n",
        "        xlabel1 = '' if y is None else f'\\nLabel: {y[j]}'\n",
        "        xlabel2 = '' if yhat is None else f'\\nPredicted: {yhat[j]}'\n",
        "        xlabel = f'{xlabel1}{xlabel2}'\n",
        "        if len(xlabel):\n",
        "            ax.set_xlabel(xlabel, fontsize=12)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "        # Plots weight as an image\n",
        "        ax.imshow(\n",
        "            np.atleast_2d(image.squeeze()),\n",
        "            cmap='gray',\n",
        "            vmin=minv,\n",
        "            vmax=maxv\n",
        "        )\n",
        "    return\n",
        "\n",
        "setattr(StepByStep, '_visualize_tensors', _visualize_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHUderM0UvZE"
      },
      "source": [
        "### Static Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCjlq-uBUvZF"
      },
      "outputs": [],
      "source": [
        "class Cat(object):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    @staticmethod\n",
        "    def meow():\n",
        "        print('Meow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJqpolXNUvZF"
      },
      "outputs": [],
      "source": [
        "Cat.meow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBBUIYfdUvZF"
      },
      "source": [
        "### Visualizing Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjO1ztaOUvZF"
      },
      "outputs": [],
      "source": [
        "weights_filter = model_cnn1.conv1.weight.data.cpu().numpy()\n",
        "weights_filter.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBzHgZVEUvZF"
      },
      "outputs": [],
      "source": [
        "def visualize_filters(self, layer_name, **kwargs):\n",
        "    try:\n",
        "        # Gets the layer object from the model\n",
        "        layer = self.model\n",
        "        for name in layer_name.split('.'):\n",
        "            layer = getattr(layer, name)\n",
        "        # We are only looking at filters for 2D convolutions\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            # Takes the weight information\n",
        "            weights = layer.weight.data.cpu().numpy()\n",
        "            # weights -> (channels_out (filter), channels_in, H, W)\n",
        "            n_filters, n_channels, _, _ = weights.shape\n",
        "\n",
        "            # Builds a figure\n",
        "            size = (2 * n_channels + 2, 2 * n_filters)\n",
        "            fig, axes = plt.subplots(n_filters, n_channels,\n",
        "                                     figsize=size)\n",
        "            axes = np.atleast_2d(axes)\n",
        "            axes = axes.reshape(n_filters, n_channels)\n",
        "            # For each channel_out (filter)\n",
        "            for i in range(n_filters):\n",
        "                StepByStep._visualize_tensors(\n",
        "                    axes[i, :],\n",
        "                    weights[i],\n",
        "                    layer_name=f'Filter #{i}',\n",
        "                    title='Channel'\n",
        "                )\n",
        "\n",
        "            for ax in axes.flat:\n",
        "                ax.label_outer()\n",
        "\n",
        "            fig.tight_layout()\n",
        "            return fig\n",
        "    except AttributeError:\n",
        "        return\n",
        "\n",
        "setattr(StepByStep, 'visualize_filters', visualize_filters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OttJ5kJSUvZF"
      },
      "outputs": [],
      "source": [
        "fig = sbs_cnn1.visualize_filters('conv1', cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI_hsduEUvZF"
      },
      "source": [
        "### Hooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHhszg-2UvZF"
      },
      "outputs": [],
      "source": [
        "dummy_model = nn.Linear(1, 1)\n",
        "\n",
        "dummy_list = []\n",
        "\n",
        "def dummy_hook(layer, inputs, outputs):\n",
        "    dummy_list.append((layer, inputs, outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTaSHwpzUvZG"
      },
      "outputs": [],
      "source": [
        "dummy_handle = dummy_model.register_forward_hook(dummy_hook)\n",
        "dummy_handle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnNO29rUUvZG"
      },
      "outputs": [],
      "source": [
        "dummy_x = torch.tensor([0.3])\n",
        "dummy_model.forward(dummy_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yjSJMycUvZG"
      },
      "outputs": [],
      "source": [
        "dummy_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe39zdJGUvZG"
      },
      "outputs": [],
      "source": [
        "dummy_model(dummy_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B69LyYWUvZG"
      },
      "outputs": [],
      "source": [
        "dummy_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm_0B3mDUvZG"
      },
      "outputs": [],
      "source": [
        "dummy_handle.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0xZdrexUvZG"
      },
      "outputs": [],
      "source": [
        "modules = list(sbs_cnn1.model.named_modules())\n",
        "modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ7x1dbIUvZG"
      },
      "outputs": [],
      "source": [
        "layer_names = {layer: name for name, layer in modules[1:]}\n",
        "layer_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrIc_JRQUvZH"
      },
      "outputs": [],
      "source": [
        "visualization = {}\n",
        "\n",
        "def hook_fn(layer, inputs, outputs):\n",
        "    name = layer_names[layer]\n",
        "    visualization[name] = outputs.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTpFmOebUvZH"
      },
      "outputs": [],
      "source": [
        "layers_to_hook = ['conv1', 'relu1', 'maxp1', 'flatten', 'fc1', 'relu2', 'fc2']\n",
        "\n",
        "handles = {}\n",
        "\n",
        "for name, layer in modules:\n",
        "    if name in layers_to_hook:\n",
        "        handles[name] = layer.register_forward_hook(hook_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j2rJ1pzUvZH"
      },
      "outputs": [],
      "source": [
        "images_batch, labels_batch = next(iter(val_loader))\n",
        "logits = sbs_cnn1.predict(images_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APlsNGWzUvZH"
      },
      "outputs": [],
      "source": [
        "visualization.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vcr2DnXUvZH"
      },
      "outputs": [],
      "source": [
        "for handle in handles.values():\n",
        "    handle.remove()\n",
        "handles = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k214Hr9KUvZH"
      },
      "outputs": [],
      "source": [
        "setattr(StepByStep, 'visualization', {})\n",
        "setattr(StepByStep, 'handles', {})\n",
        "\n",
        "def attach_hooks(self, layers_to_hook, hook_fn=None):\n",
        "    # Clear any previous values\n",
        "    self.visualization = {}\n",
        "    # Creates the dictionary to map layer objects to their names\n",
        "    modules = list(self.model.named_modules())\n",
        "    layer_names = {layer: name for name, layer in modules[1:]}\n",
        "\n",
        "    if hook_fn is None:\n",
        "        # Hook function to be attached to the forward pass\n",
        "        def hook_fn(layer, inputs, outputs):\n",
        "            # Gets the layer name\n",
        "            name = layer_names[layer]\n",
        "            # Detaches outputs\n",
        "            values = outputs.detach().cpu().numpy()\n",
        "            # Since the hook function may be called multiple times\n",
        "            # for example, if we make predictions for multiple mini-batches\n",
        "            # it concatenates the results\n",
        "            if self.visualization[name] is None:\n",
        "                self.visualization[name] = values\n",
        "            else:\n",
        "                self.visualization[name] = np.concatenate([self.visualization[name], values])\n",
        "\n",
        "    for name, layer in modules:\n",
        "        # If the layer is in our list\n",
        "        if name in layers_to_hook:\n",
        "            # Initializes the corresponding key in the dictionary\n",
        "            self.visualization[name] = None\n",
        "            # Register the forward hook and keep the handle in another dict\n",
        "            self.handles[name] = layer.register_forward_hook(hook_fn)\n",
        "\n",
        "def remove_hooks(self):\n",
        "    # Loops through all hooks and removes them\n",
        "    for handle in self.handles.values():\n",
        "        handle.remove()\n",
        "    # Clear the dict, as all hooks have been removed\n",
        "    self.handles = {}\n",
        "\n",
        "setattr(StepByStep, 'attach_hooks', attach_hooks)\n",
        "setattr(StepByStep, 'remove_hooks', remove_hooks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeoMKO90UvZH"
      },
      "outputs": [],
      "source": [
        "sbs_cnn1.attach_hooks(layers_to_hook=['conv1', 'relu1', 'maxp1', 'flatten', 'fc1', 'relu2', 'fc2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sOTpUngUvZI"
      },
      "outputs": [],
      "source": [
        "images_batch, labels_batch = next(iter(val_loader))\n",
        "logits = sbs_cnn1.predict(images_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5MP8tjtUvZI"
      },
      "outputs": [],
      "source": [
        "sbs_cnn1.remove_hooks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmECfVutUvZI"
      },
      "outputs": [],
      "source": [
        "predicted = np.argmax(logits, 1)\n",
        "predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJvrKASDUvZI"
      },
      "source": [
        "### Visualizing Feature Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAC-qxTtUvZI"
      },
      "outputs": [],
      "source": [
        "fig = plot_images(images_batch.squeeze(), labels_batch.squeeze(), n_plot=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjceHgG3UvZI"
      },
      "outputs": [],
      "source": [
        "def visualize_outputs(self, layers, n_images=10, y=None, yhat=None):\n",
        "    layers = filter(lambda l: l in self.visualization.keys(), layers)\n",
        "    layers = list(layers)\n",
        "    shapes = [self.visualization[layer].shape for layer in layers]\n",
        "    n_rows = [shape[1] if len(shape) == 4 else 1\n",
        "              for shape in shapes]\n",
        "    total_rows = np.sum(n_rows)\n",
        "\n",
        "    fig, axes = plt.subplots(total_rows, n_images,\n",
        "                             figsize=(1.5*n_images, 1.5*total_rows))\n",
        "    axes = np.atleast_2d(axes).reshape(total_rows, n_images)\n",
        "\n",
        "    # Loops through the layers, one layer per row of subplots\n",
        "    row = 0\n",
        "    for i, layer in enumerate(layers):\n",
        "        start_row = row\n",
        "        # Takes the produced feature maps for that layer\n",
        "        output = self.visualization[layer]\n",
        "\n",
        "        is_vector = len(output.shape) == 2\n",
        "\n",
        "        for j in range(n_rows[i]):\n",
        "            StepByStep._visualize_tensors(\n",
        "                axes[row, :],\n",
        "                output if is_vector else output[:, j].squeeze(),\n",
        "                y,\n",
        "                yhat,\n",
        "                layer_name=layers[i] \\\n",
        "                           if is_vector \\\n",
        "                           else f'{layers[i]}\\nfil#{row-start_row}',\n",
        "                title='Image' if (row == 0) else None\n",
        "            )\n",
        "            row += 1\n",
        "\n",
        "    for ax in axes.flat:\n",
        "        ax.label_outer()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "setattr(StepByStep, 'visualize_outputs', visualize_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UctK33sUvZI"
      },
      "outputs": [],
      "source": [
        "featurizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\n",
        "\n",
        "with plt.style.context('seaborn-white'):\n",
        "    fig = sbs_cnn1.visualize_outputs(featurizer_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FViwrxDSUvZJ"
      },
      "source": [
        "### Visualizing Classifier Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzFt1nj8UvZJ"
      },
      "outputs": [],
      "source": [
        "classifier_layers = ['fc1', 'relu2', 'fc2']\n",
        "\n",
        "with plt.style.context('seaborn-white'):\n",
        "    fig = sbs_cnn1.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKxRywUjUvZJ"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmteSSOXUvZJ"
      },
      "outputs": [],
      "source": [
        "def correct(self, x, y, threshold=.5):\n",
        "    self.model.eval()\n",
        "    yhat = self.model(x.to(self.device))\n",
        "    y = y.to(self.device)\n",
        "    self.model.train()\n",
        "\n",
        "    # We get the size of the batch and the number of classes\n",
        "    # (only 1, if it is binary)\n",
        "    n_samples, n_dims = yhat.shape\n",
        "    if n_dims > 1:\n",
        "        # In a multiclass classification, the biggest logit\n",
        "        # always wins, so we don't bother getting probabilities\n",
        "\n",
        "        # This is PyTorch's version of argmax,\n",
        "        # but it returns a tuple: (max value, index of max value)\n",
        "        _, predicted = torch.max(yhat, 1)\n",
        "    else:\n",
        "        n_dims += 1\n",
        "        # In binary classification, we NEED to check if the\n",
        "        # last layer is a sigmoid (and then it produces probs)\n",
        "        if isinstance(self.model, nn.Sequential) and \\\n",
        "           isinstance(self.model[-1], nn.Sigmoid):\n",
        "            predicted = (yhat > threshold).long()\n",
        "        # or something else (logits), which we need to convert\n",
        "        # using a sigmoid\n",
        "        else:\n",
        "            predicted = (F.sigmoid(yhat) > threshold).long()\n",
        "\n",
        "    # How many samples got classified correctly for each class\n",
        "    result = []\n",
        "    for c in range(n_dims):\n",
        "        n_class = (y == c).sum().item()\n",
        "        n_correct = (predicted[y == c] == c).sum().item()\n",
        "        result.append((n_correct, n_class))\n",
        "    return torch.tensor(result)\n",
        "\n",
        "setattr(StepByStep, 'correct', correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-7RQ-9gUvZJ"
      },
      "outputs": [],
      "source": [
        "sbs_cnn1.correct(images_batch, labels_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucYAisgoUvZJ"
      },
      "source": [
        "### Loader Apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVVBGaHDUvZJ"
      },
      "outputs": [],
      "source": [
        "@staticmethod\n",
        "def loader_apply(loader, func, reduce='sum'):\n",
        "    results = [func(x, y) for i, (x, y) in enumerate(loader)]\n",
        "    results = torch.stack(results, axis=0)\n",
        "\n",
        "    if reduce == 'sum':\n",
        "        results = results.sum(axis=0)\n",
        "    elif reduce == 'mean':\n",
        "        results = results.float().mean(axis=0)\n",
        "\n",
        "    return results\n",
        "\n",
        "setattr(StepByStep, 'loader_apply', loader_apply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAH_qwgZUvZJ"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(sbs_cnn1.val_loader, sbs_cnn1.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkiJDC_FUvZK"
      },
      "source": [
        "## Putting It All Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1_BvuJNUvZK"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS43X_P2UvZK"
      },
      "outputs": [],
      "source": [
        "# Builds tensors from numpy arrays BEFORE split\n",
        "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
        "x_tensor = torch.as_tensor(images / 255).float()\n",
        "y_tensor = torch.as_tensor(labels).long()\n",
        "\n",
        "# Uses index_splitter to generate indices for training and\n",
        "# validation sets\n",
        "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
        "# Uses indices to perform the split\n",
        "x_train_tensor = x_tensor[train_idx]\n",
        "y_train_tensor = y_tensor[train_idx]\n",
        "x_val_tensor = x_tensor[val_idx]\n",
        "y_val_tensor = y_tensor[val_idx]\n",
        "\n",
        "# We're not doing any data augmentation now\n",
        "train_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
        "val_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
        "\n",
        "# Uses custom dataset to apply composed transforms to each set\n",
        "train_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
        "val_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
        "\n",
        "# Builds a weighted random sampler to handle imbalanced classes\n",
        "sampler = make_balanced_sampler(y_train_tensor)\n",
        "\n",
        "# Uses sampler in the training set to get a balanced data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsmAYCgFUvZK"
      },
      "source": [
        "### Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D746vcdmUvZK"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "model_cnn1 = nn.Sequential()\n",
        "\n",
        "# Featurizer\n",
        "# Block 1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\n",
        "n_channels = 1\n",
        "model_cnn1.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\n",
        "model_cnn1.add_module('relu1', nn.ReLU())\n",
        "model_cnn1.add_module('maxp1', nn.MaxPool2d(kernel_size=2))\n",
        "# Flattening: n_channels * 4 * 4\n",
        "model_cnn1.add_module('flatten', nn.Flatten())\n",
        "\n",
        "# Classification\n",
        "# Hidden Layer\n",
        "model_cnn1.add_module('fc1', nn.Linear(in_features=n_channels*4*4, out_features=10))\n",
        "model_cnn1.add_module('relu2', nn.ReLU())\n",
        "# Output Layer\n",
        "model_cnn1.add_module('fc2', nn.Linear(in_features=10, out_features=3))\n",
        "\n",
        "lr = 0.1\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer_cnn1 = optim.SGD(model_cnn1.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsoQ1ALNUvZK"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTJX-sSPUvZK"
      },
      "outputs": [],
      "source": [
        "sbs_cnn1 = StepByStep(model_cnn1, multi_loss_fn, optimizer_cnn1)\n",
        "sbs_cnn1.set_loaders(train_loader, val_loader)\n",
        "sbs_cnn1.train(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GN1tfNwUvZK"
      },
      "source": [
        "### Visualizing Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gxAWhdJUvZK"
      },
      "outputs": [],
      "source": [
        "fig_filters = sbs_cnn1.visualize_filters('conv1', cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDcnQ66TUvZL"
      },
      "source": [
        "### Capturing Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zTf82lGUvZL"
      },
      "outputs": [],
      "source": [
        "featurizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\n",
        "classifier_layers = ['fc1', 'relu2', 'fc2']\n",
        "\n",
        "sbs_cnn1.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n",
        "\n",
        "images_batch, labels_batch = next(iter(val_loader))\n",
        "logits = sbs_cnn1.predict(images_batch)\n",
        "predicted = np.argmax(logits, 1)\n",
        "\n",
        "sbs_cnn1.remove_hooks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZHVnjnDUvZL"
      },
      "source": [
        "### Visualizing Feature Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "UENKv--_UvZL"
      },
      "outputs": [],
      "source": [
        "with plt.style.context('seaborn-white'):\n",
        "    fig_maps1 = sbs_cnn1.visualize_outputs(featurizer_layers)\n",
        "    fig_maps2 = sbs_cnn1.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1RBFBk2UvZL"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC0527xZUvZL"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(sbs_cnn1.val_loader, sbs_cnn1.correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMKeNTXhUvZL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}